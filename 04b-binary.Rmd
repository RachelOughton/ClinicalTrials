
### Risk Ratio (RR) and Odds ratio (OR)

The measures we have looked at so far, particularly the ARD, are quite analagous to the continuous normally distributed case. However, there are yet more commonly used measures of difference for proportions, which need to be dealt with differently,  but also afford more opportunities for modelling.

The **risk ratio** is defined as 

$$\text{RR} = \frac{\pi_T}{\pi_C}$$

The **odds ratio** is defined as 
$$\text{OR} = \frac{\pi_T/\left(1-\pi_T\right)}{\pi_C/\left(1-\pi_C\right)}$$
The first thing to note is that for both the risk ratio and the odds ratio, the null value is one (not zero, as for the ARD), and both values must always be positive. We think about things multiplicatively, so for example if $RR=3$ we can say that the event is "3 times more likely" in group $T$ than in group $C$.

#### Odds {-}

Odds and odds ratios are a bit trickier to think about ([this article](https://kids.frontiersin.org/articles/10.3389/frym.2022.926624#:~:text=As%20an%20example%2C%20if%20the,disease%20if%20you%20are%20exposed.) explains them really well - it's aimed at 'kids and teens' but don't let that put you off!). The odds of an event are the probability of it happening over the probability of it not happening. So, if (for some event $A$), $p\left(A\right)=0.2$, the odds of $A$ are 

$$\frac{p\left(A\right)}{p\left(A'\right)} = \frac{0.2}{0.8} = \frac{1}{4}, $$
which we say as "1 to 4" or 1:4. For every one time $A$ occurs, we expect it not to occur four times.

The **odds ratio** compares the odds of the outcome of interest in the Treament group with the odds of that event in the Control group. It tells us how the odds of the event are affected by the treatment (vs control).


With the ARD, we knew that our confidence interval should always be in $\left[-1,\,1\right]$, and that if we compare treatments in one direction (say $p-T - p_C$) we would obtain the negative of the interval for the other way ($p_C - p_T$). With the RR and OR, the discrepancy between two proportions is given by a ratio, and so comparing them in one direction ($p_T / p_C$) will give the reciprocal of the other direction ($p_C / p_T$).

:::{.example}

For our Streptomycin example, we estimated the ARD by 
$$\hat\tau_{ARD}=p_T - p_C = \frac{38}{55} - \frac{17}{52} = 0.364,$$
or could have alternatively had
$$\hat\tau_{ARD}=p_C - p_T = \frac{17}{52} - \frac{38}{55} = - 0.364.$$
For the risk ratio, we have

$$\hat{\tau}_{RR} = \frac{p_T}{p_C} = \frac{38/55}{17/52} = 2.113,$$
or could alternatively have

$$\hat{\tau}_{RR} = \frac{p_C}{p_T} = \frac{17/52}{38/55} = 0.473 = \frac{1}{2.113}.$$
We could say that a patient is "more than twice as likely to be cured with streptomycin than by the control". 

For the odds ratio, we have

$$\hat{\tau}_{OR} = \frac{p_T/\left(1-p_T\right)}{p_C/\left(1-p_C\right)} = \frac{(38/55)/(17/55)}{(17/52)/(35/52)} = 4.602, $$
and therefore the odds of recovery are around 4.6 greater for Streptomycin than for the control. Similarly, we could reframe this as

$$\hat{\tau}_{OR} = \frac{p_C/\left(1-p_C\right)}{p_T/\left(1-p_T\right)} = \frac{(17/52)/(35/52)}{(38/55)/(17/55)} = 0.217 = \frac{1}{4.602}.$$

:::

One thing to notice is that symmetry works differently on the RR and OR scale from on the ARD scale. There is an equivalence between an interval $\left(l,\,u\right)$ (with $l,u>1$) and $\left(\frac{1}{u},\frac{1}{l}\right)$, since these intervals would equate to comparing the same two treatments in different directions (assuming the difference was significant and neither interval contains 1). Similarly, on this scale the interval 

$$\left(\frac{1}{k},\,k\right) \text{ for some }k>1 $$
can be thought of as symmetric, in that one treatment may be up to $k$ times more effective than the other, in either direction. Therefore, to build a confidence interval for OR or RR, we will not be following the usual formula

$$\text{point estimate } \pm{z\times{SE}}.$$
You may have already been thinking that a log transformation would be useful here, and you'd be correct! The *sort-of* symmetric intervals we've been discussing here actually are symmetric (about zero) on the log scale.


#### Confidence intervals for RR and OR

Firstly we'll consider the risk ratio. Let's define 

$$ \phi = \log\left(\frac{\pi_T}{\pi_C}\right).$$
The natural way to estimate this is with the sample proportions

$$\log\left(\frac{p_T}{p_C}\right) = \log\left(p_T\right) - \log\left(p_C\right).$$
These estimated proportions should be approximately normal and independent of one another, and so $\log\left(\frac{p_T}{p_C}\right)$ is approximately normal with mean $\phi$ (the true value) and variance 

$$\operatorname{var}\left(\log\left(p_T\right)\right) + \operatorname{var}\left(\log\left(p_C\right)\right). $$
We can now apply the Delta method (see section \@ref(delta-method)) to find that (using Equation \@ref(eq:delta3))

$$\operatorname{var}\left[\log\left(p_T\right)\right] = \operatorname{var}\left[\log\left(\frac{r_T}{n_T}\right)\right] \approx \frac{\pi_T\left(1-\pi_T\right)}{n_T}\times{\left(\frac{1}{\pi_T}\right)^2} = \frac{1}{n_T\pi_T} - \frac{1}{n_T}. $$
Since we estimate $\pi_T$ by $r_T/n_T$ this can be estimated by $r_T^{-1} - n_T^{-1}$. Notice that we are relying on the derivative of $\log\left(x\right)$ being $x^{-1}$, so we must always use natural logarithms.

This leads us to the result that, approximately

$$\log\left(\frac{p_T}{p_C}\right) \sim N\bigg(\phi,\,\left(r_T^{-1} - n_T^{-1}\right) + \left(r_C^{-1} - n_C^{-1}\right) \bigg) $$ and so we can generate $100\left(1-\alpha\right)$% confidence intervals for $\phi$ as $\left(l_{RR},\;u_{RR}\right)$, where the limits are

$$
\log\left(\frac{p_T}{p_C}\right) \pm z_{\frac{\alpha}{2}}\sqrt{\left(r_T^{-1} - n_T^{-1}\right) + \left(r_C^{-1} - n_C^{-1}\right)}.
$$
This then translates to an interval for the risk ratio itself of $\left(e^{l_{RR}},e^{u_{RR}}\right)$.

:::{.example}
Returning once again to our streptomycin example, recall that we have 

$$
\begin{aligned}
r_T & = 38\\
n_T & = 55 \\
r_C & = 17 \\
n_C & = 52
\end{aligned}
$$
and so the limits of the confidence interval (with $\alpha=0.05$) on the log scale are

$$\log\left(\frac{38/55}{17/52}\right) \pm 1.96\sqrt{\frac{1}{38} - \frac{1}{55} + \frac{1}{17} - \frac{1}{52}} = \log(2.11) \pm 1.96 \times 0.218$$

which gives us $\left(0.320,\,1.176\right)$ on the log scale, and a 95% CI for the risk ratio of $\left(1.377,\,3.243\right)$.

:::



#### Summary {-}

We can collect all our confidence intervals together
DO I WANT TO?! YES, I MIGHT AS WELL - P 109.


## Accounting for baseline observations: logistic regression

We saw with the continuous outcomes that it is often advantageous to include baseline measurements of the outcome (if they are known) in our analysis, and this is the same for binary outcomes. 

In this section we use the term 'baseline observations' to mean any measurement that was known before the trial started. Unlike with continuous measurements, with a binary outcome, there is not usually a pre-trial value of the primary outcome. A binary outcome is often already relative to pre-trial (for example 'Have the patient's symptoms improved?') or refers to an event that definitely wouldn't have happened pre-trial (for example 'Did the patient die within the next 6 months?' or 'Was the patient cured?'). However, as we saw with ANCOVA, we can include other sorts of covariates in a linear model, so this is fine.

The general form of model that we would like for patient $i$ is

$$\text{outcome}_i = \mu + \tau G_i + \beta_1\times{\text{baseline}_{1i}} + \ldots + \beta_p\times{\text{baseline}_{pi}} + \text{error}_i,$$
where $G_i$ is an indicator function taking values 1 if patient $i$ was in group $T$ and 0 if they were in group $C$, and $\text{baseline}_1,\;\ldots,\;\text{baseline}_p$ are $p$ baseline measurements that we would like to take into account.

However, this actually creates quite a few problems with binary variables. The outcome for patient $i$ will be either 0 or 1, but the terms in the model above do not guarantee this at all. Adding a normally distributed error term doesn't really make sense in this context, so we will remove it. We can also make the LHS more continuous by thinking of the mean outcome rather than a single outcome. This makes sense, since if several patients were identical to patient $i$ (in the sense of having the same baseline covariate values and being allocated to the same treatment), we probably wouldn't expect them all to have exactly the same outcome. Therefore we might instead think in terms of mean outcome, in which case our model becomes

$$\text{mean outcome}_i = \mu + \tau G_i + \beta_1\times{\text{baseline}_{1i}} + \ldots + \beta_p\times{\text{baseline}_{pi}}.$$

There is one final problem to overcome, which is that the LHS will certainly be in $\left[0,\;1\right]$, but the RHS could take any value. To address this we need to use a transformation, to take the mean outcome from $\left[0,1\right]$ to $\mathbb{R}$.

The transformation that is usually used for a binary variable is the **logit** function, which is the log of the odds,

$$\operatorname{logit}\left(\pi\right) = \log\frac{\pi}{1-\pi}.$$

As $\pi$ tends to zero, $\operatorname{logit}\left(\pi\right)$ tends to $-\infty$, and as $\pi$ tends to one, $\operatorname{logit}\left(\pi\right)$ tends to $\infty$. The derivative of the $\operatorname{logit}$ function is

$$ \frac{d\operatorname{logit}\left(\pi\right)}{d\pi} = \frac{1}{\pi\left(1-\pi\right)}$$
which is always positive for $\pi\in\left[0,1\right]$. This means that we can use it to transform our mean outcome (which we will now call $\pi$, since the mean outcome is the estimate of the probability of success) in the model

\begin{equation}
\operatorname{logit}\left(\pi\right) = \mu + \tau G_i + \beta_1\times{\text{baseline}_{1i}} + \ldots + \beta_p\times{\text{baseline}_{pi}}
(\#eq:logreg1)
\end{equation}

and any value in $\mathbb{R}$ is allowed on both sides. This model is known as **logistic regression**, and belongs to a class of models called **Generalized Linear Models**. If you did Advanced Statistical Modelling III you'll have seen these before. If you haven't seen them, and want to know more, [this article](https://www.r-bloggers.com/2015/08/generalised-linear-models-in-r/) gives a nice introduction (and some useful R tips!).

  
### What does this model tell us?

We now have an equation for a model that makes sense, but what is it actually modelling? And what does it tell us about the effect of the treatment?
Consider the difference between two patients who are the same in every respect except one is assigned to group $C$ (so $G=0$) and the other to group $T$ (so $G=1$). The model gives:

$$
\begin{aligned}
\operatorname{logit}\left(\pi\right) = \log\left(\frac{\pi}{1-\pi}\right) = \log\left(\text{Odds of success}\right) & = \mu + \tau + \beta_1x_1 + \ldots + \beta_px_p & \text{ (group T)}\\
\operatorname{logit}\left(\pi\right) = \log\left(\frac{\pi}{1-\pi}\right) = \log\left(\text{Odds of success}\right) & = \mu + \beta_1x_1 + \ldots + \beta_px_p & \text{ (group C)}
\end{aligned}
$$
Subtracting one from the other, we find

$$
\begin{aligned}
\log(\text{Odds of success for group T}) - & \log(\text{Odds of success for group C})\\
&= 
\log\left(\frac{\text{Odds of success for group T}}{\text{Odds of success for group C}}\right) = \log\left(OR\right) \\
&= \tau.
\end{aligned}
$$

That is, $\tau$ is the log of the odds ratio, or $e^\tau$ is the odds ratio of success in group $T$ relative to group $C$, adjusted for variables $x_1,\;\ldots,\;x_p$. Put another way, while the baseline covariates $x_1,\ldots,x_p$ affect the probability of 'success' (or whatever our binary outcome's one means), $\tau$ is a measure of the effect of the treatment compared to control given some set of baseline covariate values. 

### Fitting a linear regression model 

Logistic regression models are generally fitted using *maximum likelihood*. In the notation of Equation \@ref(eq:logreg1), the parameters we need to fit are the coefficients $\mu,\;\tau$ and $\beta_1,\ldots,\beta_p$. To ease notation, we will collect these into a vector $\boldsymbol\beta$, with $\beta_0=\mu$, $\beta_1=\tau$ and $\beta_2,\ldots,\beta_{p+1}$ the original $\beta_1,\ldots,\beta_p$. Sorry this is confusing - we won't really use the vector $\boldsymbol\beta$ after this, or think about the parameters individually (apart from $\tau$).

This notation allows us to write the linear function on the RHS of Equation \@ref(eq:logreg1) for participant $i$ as 

$$x_i^T\boldsymbol\beta = \sum\limits_{j=0}^{q} x_{ij}\beta_j, $$
where 

  * $x_{i0}=1$ (so that $\beta_0$ is the intercept $\mu$)
  * $x_{i1}=
  \begin{cases}
  0\text{ if participant }i\text{ is in group }C\\
  1\text{ if participant }i\text{ is in group }T
  \end{cases}$
  * $x_{i2},\ldots,x_{iq}$ are the baseline covariates.
  
If $\pi_i$ is the probability that the outcome for participant $i$ is 1, where $i=1,\ldots,n$, then the logistic model specifies these $n$ parameters through the $q+1$ parameters $\beta_j$, via the $n$ expressions 

\begin{equation}
\operatorname{logit}\left(\pi_i\right) = x_i^T\boldsymbol\beta.
(\#eq:logit1)
\end{equation}

Using the Bernoulli distribution, the log-likelihood of the data is

\begin{align*}
\ell\left(\left\lbrace\pi_i \right\rbrace \mid\text{data}\right) & = \sum\limits_{i=1}^n\left[y_i\log(\pi_i) + \left(1-y_i\right)\log\left(1-\pi_i\right)\right]\\
& = \sum\limits_{i=1}^n\left[y_i\log\left(\frac{\pi_i}{1-\pi_i}\right) + \log\left(1-\pi_i\right)\right],
\end{align*}
where $y_i=0$ or 1 is the outcome for participant $i$. Using Equation \@ref(eq:logit1) we can rewrite this in terms of $\boldsymbol\beta$ as 

$$\ell\left(\left\lbrace\beta_j \right\rbrace\mid{\text{data}}\right) = \sum\limits_{i=1}^n \left[y_i x_i^T\boldsymbol\beta - \log\left(1+e^{x_i^T\boldsymbol\beta}\right)\right].$$

The fitted model is then the one with the values $\beta_j$ that maximise this expression (and hence maximise the likelihood itself), which we will label the $\left\lbrace \hat{\beta}_j\right\rbrace$. 

This is generally done some via some numerical method, and we won't go into that here. The method used by R will generate the MLE $\hat\beta_j$ for each $\beta_j$, and also an estimate of the standard error of the estimate. In particular there will be an estimate of the standard error of $\hat\beta_1$, better known as $\hat\tau$, the estimate of the treatment effect. This is important, because it means we can test the hypothesis that $\tau=0$, and can form a confidence interval for the adjusted log OR.

:::{.example #logregeg1}

This study is detailed in @elmunzer2012randomized. ERCP, or endoscopic retrograde cholangio-pancreatogram, is a procedure performed by threading an endoscope through the mouth to the opening in the duodenum where bile and pancreatic digestive juices are released into the intestine. ERCP is helpful for treating blockages of flow of bile (gallstones, cancer), or diagnosing cancers of the pancreas, but has a high rate of complications (15-25%). The occurrence of post-ERCP pancreatitis is a common and feared complication, as pancreatitis can result in multisystem organ failure and death, and can occur in ~ 16% of ERCP procedures. This study tests whether the use of anti-inflammatory NSAID therapies at the time of ERCP reduce the rate of this complication. The study had 602 participants.

The dataset contains 33 variables, but we will focus on a small number:

  * $X$: (primary outcome) - incidence of post-ercp pancreatitis 0 (no), 1 (yes).
  * Treatment arm `rx`: 0 (placebo), 1 (treatment)
  * Site: 1, 2, 3, 4
  * Risk: Risk score (1 to 5). Should be factor but treated as continuous.
  * Age: from 19 to 90, mean 45.27, SD 13.30.

The correlation between `risk` and `age` is -0.216, suggesting no problems of collinearity between those two variables.


```{r, echo=T}
data("indo_rct")
summary(indo_rct)
## Some things to note:
# There are very few patients in group 4, and not many in group 3
# The age range goes from 19 to 90 
# 'rx' is the group variable

## Checking for collinearity with factor variables

# No consistent patterns between age and site or risk and site
indo_rct%>%
  group_by(site) %>% 
  summarise(
    meanage=mean(age), sdage=sd(age),
    meanrisk = mean(risk), sdrisk=sd(risk)
    )

## We will try models with age and age^2

glm_indo_agelin = glm(outcome ~ age + site + risk + rx, data=indo_rct, family = binomial(link = "logit"))
glm_indo_agesq = glm(outcome ~ I(age^2) + site + risk + rx, data=indo_rct, family = binomial(link = "logit"))

summary(glm_indo_agelin)
summary(glm_indo_agesq)

```

Since neither `age` nor `age^2` appear influential, we'll remove it and keep the other covariates.

```{r, echo=T}
glm_indo = glm(outcome ~ site + risk + rx, data=indo_rct, family = binomial(link = "logit"))
summary(glm_indo)
```

From the summary we see that $\hat\tau = -0.752$, with a standard error of 0.261. A 95% CI for $\tau$ is therefore 

$$-0.752 \pm 1.96\times 0.261 = \left(-1.26,\;-0.240\right).$$
:::

We can also use the model to estimate the odds of 'success' (the outcome 1) for different groups of patients, by fixing the values of the covariates. The linear expression $x\boldsymbol\hat\beta$ for given values of $x$ gives us as estimate of 

$$\log\left(\frac{p(X=1)}{1-p(X=1)}\right),$$ 
where $X$ here is the primary outcome. The exponent of this therefore gives the odds.

:::{.example}
Continuing with Example \@ref(exm:logregeg1), we can find estimates of the log odds (and therefore the odds) of post-ECRP pancreatitis for various categories of patient.

For this we will make heavy use of the summary table

```{r, echo=T}
summary(glm_indo)
```

For example, a patient from site 1, with risk level 3, in the control group would have odds

$$\exp\left(-2.2307 + 3\times 0.5846\right) = 0.6207, $$
which translates to a probability of 

$$\frac{0.6207}{1+0.6207} = 0.383. $$

By contrast, a patient in group $T$, from site 2, at risk level 1, would have odds

$$\exp\left(-2.2307 - 1.2204 + 1\times 0.5846 - 0.7523\right) = 0.0268, $$
which is equivalent to a probability of post-ECRP pancreatitis of 

$$\frac{0.0268}{1+0.0268} = 0.0261.$$ 
Being more methodical we can collect these into a table. Since the site 3 and 4 coefficents are not significant (mainly due to a lack of data), we will treat them as zero and lump them in with the site 1 people.

```{r}
sites = c("2", "Not 2")
risks = 1:5

df_indo = data.frame(
  site = c(rep("2", 5), rep("Not 2", 5)),
  risk = rep(1:5, 2)
)

df_indo$Odds_groupC = rep(NA, 10)
df_indo$Prob_groupC = rep(NA, 10)
df_indo$Odds_groupT = rep(NA, 10)
df_indo$Prob_groupT = rep(NA, 10)

for (i in 1:5){
  oddsC = exp(-2.2307 - 1.2204 + df_indo$risk[i]*0.5846)
  oddsT = exp(-2.2307 - 1.2204 + df_indo$risk[i]*0.5846 - 0.7523)
  df_indo$Odds_groupC[i] = round(oddsC,3)
  df_indo$Odds_groupT[i] = round(oddsT,3)
}

for (i in 1:5){
  oddsC = exp(-2.2307  + df_indo$risk[i]*0.5846)
  oddsT = exp(-2.2307  + df_indo$risk[i]*0.5846 - 0.7523)
  df_indo$Odds_groupC[i+5] = round(oddsC,3)
  df_indo$Odds_groupT[i+5] = round(oddsT,3)
}

df_indo$Prob_groupC = round(df_indo$Odds_groupC/(1+df_indo$Odds_groupC),3)
df_indo$Prob_groupT = round(df_indo$Odds_groupT/(1+df_indo$Odds_groupT),3)

df_indo
```

:::

#### Some cautions {-}

As with any linear model, we need to ensure that it is appropriate for our dataset. Two key things we need to check for are:

  * **Collinearity**: we should make sure that none of the independent variables are highly correlated. This is not uncommon in clinical datasets, since measurements are sometimes strongly related. Sometimes therefore, this can mean choosing only one out of a collection of two or more strongly related variables.
  * **linear effect across the range of the dataset**: a linear model is based on the assumption that the effect of the independent variables is the same across the whole range of the data. This is not always the case. For example, the rate of deterioration with age can be more at older ages. This can be dealt with either by binning age into categories, or by using a transformation, eg. age$^2$. Note that this would still be a linear model, because it is linear in the coefficients.




## Diagnostics for logistic regression

There are many diagnostic techniques for binomial data (see eg. @collett_bin) but we will only touch on a small number. Unlike with a linear regression model, we don't have residuals to analyse. Our model outputs probabilities, but our data is all either 0 or 1. This makes diagnostics somewhat trickier.

Diagnostics for logistic regression fall into two categories: **discrimination** and **calibration**. We will look at each of these in turn.

### Discrimination

Here we are thinking of the logistic regression model as a classifier: for each participant the model outputs some value, on the $\operatorname{logit}\left(p\right)$ scale. If that value is below some threshold, we classify that participant as 0 If the value is above the threshold, we classify them as 1. 


### Calibration





If the explanatory variables are factors, and we have repeated observations for the different combinations of factor levels, then for each combination we can estimate the probability of success (or whatever our outcome variable is) using the data, and compare this to the fitted model value.


:::{.example}

This example uses the model fitted in Example \@ref(exm:logregeg1).
  
The trial has 602 participants and there are many fewer than 602 combinations of the above factor variables, so for many such combinations we will have estimates. Since we are in three dimensions, plotting the data is moderately problematic. We will have a plot for each site (or for the two main ones), use risk score for the $x$ axis and colour points by treatment group. The circles show the estimates from the data, and are sized by the number of observations used to calculate that estimate, and the crosses and lines show the mean and 95% CI of the fitted value. 

```{r}

indo_exp = indo_rct[,c(2,4,32)]
indo_fact = unique(indo_exp)
fit_fact = predict(glm_indo, newdata=indo_fact, se.fit=T, type="response")
indo_fact$fit = fit_fact$fit
indo_fact$fit_se = fit_fact$se.fit
indo_fact$est = rep(NA, nrow(indo_fact))
indo_fact$size = rep(NA, nrow(indo_fact))
for (i in 1:nrow(indo_fact)){
  indo_sub = indo_rct[
    (indo_rct$site == indo_fact$site[i])&(indo_rct$risk == indo_fact$risk[i])&(indo_rct$rx == indo_fact$rx[i]),
  ] 
  indo_fact$est[i] = (sum(indo_sub$outcome=="1_yes"))/nrow(indo_sub)
  indo_fact$size[i] = nrow(indo_sub)
}
# indo_fact
# By group
plot0 = ggplot(data=indo_fact[indo_fact$rx=="0_placebo",], aes(x=risk, col=site)) +
  geom_point(aes(y=est, size=size), pch=16) + 
  geom_point(aes(y=fit), pch=4) +
  geom_segment(aes(x=risk, xend=risk, y=fit-1.96*fit_se, yend=fit+1.96*fit_se))+
  theme_bw()+ggtitle("Control Group")

plot1 = ggplot(data=indo_fact[indo_fact$rx=="1_indomethacin",], aes(x=risk, col=site)) +
  geom_point(aes(y=est, size=size), pch=16) + 
  geom_point(aes(y=fit), pch=4) +
  geom_segment(aes(x=risk, xend=risk, y=fit-1.96*fit_se, yend=fit+1.96*fit_se))+
  theme_bw() + ggtitle("Treatment Group")

# By site

plot_s1 = ggplot(data=indo_fact[indo_fact$site=="1_UM",], aes(x=risk, col=rx)) +
  geom_point(aes(y=est, size=size), pch=16) + 
  geom_point(aes(y=fit), pch=4) +
  geom_segment(aes(x=risk, xend=risk, y=fit-1.96*fit_se, yend=fit+1.96*fit_se))+
  theme_bw()+ggtitle("Site 1")
plot_s2 = ggplot(data=indo_fact[indo_fact$site=="2_IU",], aes(x=risk, col=rx)) +
  geom_point(aes(y=est, size=size), pch=16) + 
  geom_point(aes(y=fit), pch=4) +
  geom_segment(aes(x=risk, xend=risk, y=fit-1.96*fit_se, yend=fit+1.96*fit_se))+
  theme_bw()+ggtitle("Site 2")

grid.arrange(plot_s1, plot_s2, nrow=2)


```


:::

#### Separation plots

In a linear regression, we can plot the residual (the true value minus the fitted value) to see whether it fits our modelling assumptions that it should be normally distributed and independent of the fitted values and of all covariates. In a logistic regression model, our actual values are 0 and 1, and we have no error term. The uncertainty in the model comes from the fact that it is estimating probabilities (via the logit link function) rather than directly modelling the outcome.

Separation plots are an attempt to visualise each piece of data against the model. The dataset is reordered according to the fitted value of the probability, and the fitted probability is plotted as a line. A vertical line (red, in the `separationplot` package) is drawn for every point where the outcome is 1, and a different coloured line (beige in `separationplot`) for every outcome that is 0. There are two main useful things the plot can show you:

  1. **The better the separation, the better the model**. The first plot shows a reasonably good model, where the density of red lines approximately follows the line of fitted probability. However, the line is fairly shallow, and the red lines are fairly scattered. The second plot shows a perfect model. The fitted probability jumps from 0 to 1 at around 0.7 (shown by the black triangle), and there is perfect separation between the ones and zeroes.
  
```{r}
fit1 <- glm(inmetro ~ percollege, data = midwest, family = binomial)
midwest$f1 <- predict(fit1, midwest, type = "response")
separationplot(pred = midwest$f1, actual = midwest$inmetro, show.expected = T)
```

```{r}
fit3 <- glm(inmetro ~ category, data = midwest, family = binomial)
midwest$f3 <- predict(fit3, midwest, type = "response")
separationplot(midwest$f3, midwest$inmetro, show.expected = T)

```

  2. **If the data doesn't follow the line, the fit is off**. 
  
```{r}
mw_eg = data.frame(
  inmetro = midwest$inmetro,
  percollege = midwest$percollege,
  unif1 = runif(n=nrow(midwest), min=0, max=1),
  cat1 = sample(c(0,1,2,3), size=nrow(midwest), replace=T)
  
)
glm_eg = glm(inmetro ~ ., data=mw_eg, family=binomial(link = 'logit'))
mw_eg$fit <- predict(glm_eg, mw_eg, type = "response")
separationplot(mw_eg$fit, mw_eg$inmetro, show.expected = T)
summary(glm_eg)
```
  
You can see a nice example of using separation plots [here](https://www.ndrewwm.com/post/20191023-separation-plots/) (note that they don't use the library we'll use though). 

For the effect of a binary variable on the log of the odds ratio, we have
$$\hat{\beta_i} \pm z_{\alpha/2}\sqrt{\hat{v_i}},$$
where $\hat\beta_i$ is the estimated coefficient of covariate $i$, and $\sqrt{\hat{v}_i}$ is the standard error of the estimate. This can then be transformed into a confidence interval for the odds ratio.



:::{.example}
```{r}
library(HSAUR)
data("respiratory")
resp_04 = respiratory[respiratory$month %in% c(0,4),]
resp_4 = respiratory[respiratory$month %in% c(4),]
resp_4$status0 = resp_04$status[resp_04$month==0]
```
The data in this example is from a trial in which a drug is being tested for whether it improves the conditions of a respiratory condition. For each patient, we have the following baseline covariates:

  * sex 
  * age 
  * treatment centre (centre 1 or centre 2)
  * age 
  * symptom status (poor = 0, good = 1).
  
The outcome variable is whether the status of the patient's symptoms are poor (0) or good (1) after four months of the trial. The first model we fit involves all covariates:

```{r, echo=T}
model1 = glm(status ~ centre + treatment + sex + age + status0, 
             family = binomial(link='logit'), data=resp_4)
summary(model1)
```
And we can plot the separation plot.
```{r, echo=T}
fit1 = predict(model1, resp_4, type = "response")
separationplot(fit1, (as.numeric(resp_4$status)-1) )
```

Having done this we can fit a second model with only those covariates that appear to be significantly active:

```{r, echo=T}
model2 = glm(status ~ centre + treatment +  status0, 
             family = binomial(link='logit'), data=resp_4)
summary(model2)
```
The separation plot shows that the model is OK (though certainly not brilliant).
```{r}
fit2 = predict(model2, resp_4, type = "response")
separationplot(fit2, (as.numeric(resp_4$status)-1) )

```

First of all, we see that the treatment is significant, and that all other things being equal, being in the treatment group increases the log of the odds ratio by around 1.024. We can construst a 95% confidence interval for this using the estimate and standard error of the coefficient (shown in the R output above), 

$$1.024 \pm 1.96 \times{0.453} = \left(0.136,\; 1.912\right).$$
Taking the exponent, the 95% confidence interval for the effect of the treatment on the **odds** of 'good' symptom status at 4 months is
$$\left(\exp(0.136),\; \exp(1.912)\right) = \left(1.145,\;6.768\right).$$

Using the coefficient estimates from `model2` above, we see 

$$\log\frac{\pi}{1-\pi} = -1.643 + 1.101\left(\text{centre}=2\right) + 1.024\left(\text{treatment}=1\right) + 1.729\left(\text{baseline status}=1\right), $$
where $\pi$ is the probability of the symptom status being 'good' (1) at four months.
The odds of the outcome being 1 can be estimated from this equation by taking the exponent. For example, for a patient at treatment centre 2, in the treatment group, with 'good' baseline status, the odds of a 'good' status at 4 months are approximately

$$
\begin{aligned}
\frac{\pi}{1-\pi} & =\exp\left[ -1.643 + 1.101 + 1.024 + 1.729\right] \\
& = \exp\left(2.211\right)\\
& = 9.125
\end{aligned}
$$
which corresponds to a probability of a 'good' status at four months of 0.901. 

By contrast, for a patient in the treatment group at treatment centre 1, who had 'poor' symptoms at baseline, the odds of a 'good' status at 4 months are approximately

$$
\begin{aligned}
\frac{\pi}{1-\pi} & =\exp\left[ -1.643 + 1.024 \right] \\
& = \exp\left(-0.619\right) \\
& = 0.538.
\end{aligned}
$$
Rearranging this for probability we find $\pi = 0.350$.

However, if that same participant had been in the control group instead, we would have

$$
\begin{aligned}
\frac{\pi}{1-\pi}& = \exp\left(-1.643\right)\\
& = 0.193
\end{aligned}
$$
and the estimated probability of having 'good' symptom status at four months would be 0.162.

:::






