# Bias
In statistics, *bias* is a systematic tendency for the results of our analysis to be different from the true value. We see this particularly when we are using sample data to estimate a parameter. We will revisit what we have learned in previous courses about bias before going on to see how it affects RCTs.

::: {.definition name="Bias of an estimate"}
Suppose that $T$ is a statistic calculated to estimate a parameter $\theta$. The **bias** of $T$ is $$E\left(T\right) - \theta.$$ If the bias of $T$ is zero, we say that $T$ is an **unbiased estimator** of $\theta$.
:::
  
  An example you will have seen before is the standard deviation. If we have some data $x_1,\,\ldots,x_n$ that are IID $N\left(\mu,\,\sigma^2\right)$, we can calculate the sample variance 

$$ s^2 = \frac{1}{n}\sum\limits_{i=1}^n\left(x_i - \bar{x}\right)^2 .$$
  
  In this case, $E\left(s^2\right) \neq {\sigma^2}$, and $s^2$ is an biased estimator. However, we know that

$$E \left(\frac{n}{n-1}s^2\right) = \sigma^2,$$
  and therefore we can apply this correction to the sample variance $s^2$ to produce an unbiased estimate of the population variance $\sigma^2$.

However, suppose our sample $x_1,\ldots,x_n$ were drawn from $N\left(\mu,\sigma^2\right)$, but were not independent of one another. Then, neither our estimator $s^2$, nor our bias-corrected estimator $\frac{n}{n-1}s^2$ would have expected value $\sigma^2$. Furthermore, we cannot use our sample $x_1,\ldots,x_n$ to produce an unbiased estimator of $\sigma^2$, or even of the mean $\mu$.

This scenario is much closer to what we mean when we talk about *bias* in a clinical trial setting. Suppose we are testing some new treatment $A$ against the standard $B$. We measure some outcome $X$ for each patient, and our hypothesis is that $X$ behaves differently for those in the treatment group than for those in the control group. It is common practice to express this additively, 
$$E\left(X\right) = \mu + \tau,$$
  where $\tau$ is our treatment effect, which we can estimate using the difference in the groups' means, $\bar{X}_B - \bar{X}_A$. Our null hypothesis is that $\tau = 0$, and our alternative hypothesis is that $\tau\neq{0}$, and therefore an estimate of $\tau$ from our data is very important! Put equivalently, it is important that there is no bias in our estimates of $\bar{X}_A$ and $\bar{X}_B$.
                                                                                         
Usually, what this comes down to is that the assumption that the data are independent, identically distributed random variables from the relevant distributions (which we have already relied on a lot for our sample size calculations) has been violated in some way. 
                                                                                         
Many of the ideas in this section are closely linked to the topic of **allocation**, which we will discuss in detail in Section \@ref(sec-allocation)).

## Where does bias come from?

Having established that bias is a serious issue in clinical trials, we will think about several sources of bias. Some of these we will elaborate on as we get to the relevant part of methodology. Most sources of bias creep in during the allocation or selection phase.

### Selection bias

Selection bias occurs when certain patients or subjects are systematically more (or less) likely be entered into the trial because of the treatment they will receive. This shouldn't be able to happen, because it is usually only after a participant has been recruited that 
their treatment is chosen. If a medical professional is not comfortable with a particular patient potentially receiving one of the possible treatments, then that patient should not be entered into the trial at all. If there are many such [technically eligible] patients, then this might mean that the estimated treatment effect is worryingly far from the true population treatment effect (where the population is the group of all eligible patients), but this is not technically selection bias.

It may happen that the doctor knows which treatment a patient would be given, for example if the allocation follows some deterministic pattern, or is fully known to the doctor in advance . Consciously or subconsciously this knowledge may influence the description they give to potential participants, and this in turn may affect which patients sign up, and the balance of the groups. In practice there should be various safeguards against this situation.

:::{.example}

Suppose we run a trial comparing a surgical (S) and a non-surgical (N) treatment for some condition. Patients who are eligible are given the opportunity to join the trial by a single doctor.

The severity of each participants disease is graded as 1 (less serious) or 2 (more serious). Across the full group of participants, proportion $\lambda$ have severity 1 and proportion $1-\lambda$ have severity 2. 

Our primary outcome is survival time, $X$, which depends on the severity of disease:
  
  \begin{align*}
E\left(X\mid{1}\right) & = \mu_1\\
E\left(X\mid{2}\right) & = \mu_2
\end{align*}

and we assume $\mu_1>\mu_2$.

For the overall trial group, for untreated patients we have

$$ E\left(X\right) = \mu = \lambda \mu_1 + \left(1-\lambda\right)\mu_2.$$
  Suppose that for treatment group $N$, the expected survival time increase by $\tau_N$, and similarly for group $S$, so that we have

\begin{align*}
E\left(X\mid{N,1}\right) & = \mu_1 + \tau_N\\
E\left(X\mid{N,2}\right) & = \mu_2 + \tau_N\\
E\left(X\mid{S,1}\right) & = \mu_1 + \tau_S\\
E\left(X\mid{S,2}\right) & = \mu_2 + \tau_S.
\end{align*}

If all patients were admitted with equal probability to the trial (ie. independent of the severity of their disease) then the expected survival time for group $N$, $E\left(X\mid{S}\right)$,  would be

\begin{align*}
E\left(X\mid{1,N}\right)P\left(1\mid{N}\right) +  E\left(X\mid{2,N}\right)P\left(2\mid{N}\right)& = \left(\mu_1 + \tau_N\right)\lambda + \left(\mu_2+\tau_N\right)\left(1-\lambda\right)\\
& = \mu + \tau_N.
\end{align*}

Similarly, the expected survival time in group $S$ would be $\mu+\tau_S$, and the treatment effect difference between the two would be $\tau = \tau_N - \tau_S$ and the trial is unbiased.

Suppose that although all eligible patients are willing to enter the trial, the doctor is reticent to subject patients with more severe disease (severity 2) to the surgical procedure. This is reflected in the way they explain the trial to each patient, particularly those with severity 2 whom the doctor knows will be assigned to group $S$. In turn this leads to a reduced proportion $q = 1-p$ of those with severity 2 assigned to  surgery entering the trial (event $A$):
  
  \begin{align*}
P\left(A\mid{N,1}\right) = P\left(A\mid{S,1}\right) = P\left(A\mid{N,2}\right) & = 1 \\
P\left(A\mid{S,2}\right) & = 1-p = q.
\end{align*}

Since our analysis is based only on those who enter the trial, our estimated treatment effect will be 

$$E\left(X\mid{A, N}\right) - E\left(X\mid{A, S}\right). $$
  We can split these according to disease severity, so that

$$E\left(X\mid{A,N}\right) = E\left(X\mid{A,N,1}\right)P\left(1\mid{A,N}\right) + E\left(X\mid{A,N,2}\right)P\left(2\mid{A,N}\right) $$
  and similarly for group $S$.

We can calculate $P\left(1\mid{A,N}\right)$  using Bayes' theorem,

\begin{align*}
P\left(1\mid{A,N}\right) & =  \frac{P\left(A\mid{1,N}\right)P\left(1\mid{N}\right)}{P\left(A\mid{N}\right)}\\
& = \frac{P\left(A\mid{1,N}\right)P\left(1\mid{N}\right)}{P\left(A\mid{N,1}\right)P\left(1\mid{N}\right) + P\left(A\mid{N,2}\right)P\left(2\mid{N}\right)} \\
&= \frac{1\times{\lambda}}{1\times {\lambda} + 1 \times{\left(1-\lambda\right)}}\\
& = \lambda.
\end{align*}
Therefore we also have $P\left(2\mid{A,N}\right) = 1 -P\left(1\mid{A,N}\right) =  1-\lambda$.

Following the same process for group $S$, we arrive at 

\begin{align*}
P\left(1\mid{A,S}\right) & =  \frac{P\left(A\mid{1,S}\right)P\left(1\mid{S}\right)}{P\left(A\mid{S}\right)}\\
& = \frac{P\left(A\mid{1,S}\right)P\left(1\mid{S}\right)}{P\left(A\mid{S,1}\right)P\left(1\mid{S}\right) + P\left(A\mid{S,2}\right)P\left(2\mid{S}\right)} \\
& = \frac{\lambda}{\lambda + q\left(1-\lambda\right)}
\end{align*}.

Notice that $P\left(2\mid{S}\right)= 1-\lambda$, since it is not conditional on actually participating in the trial. Therefore,

\begin{align*}
E\left(X\mid{A,N}\right) & = E \left(X\mid{N,1}\right)P\left(1\mid{A,N}\right) + E \left(X\mid{N,2}\right)P\left(2\mid{A,N}\right) \\
& = \left(\mu_1 + \tau_N\right)\lambda + \left(\mu_2 + \tau_N\right)\left(1-\lambda\right) \\
& = \lambda\mu_1 + \left(1-\lambda\right)\mu_2 + \tau_N
\end{align*}

and 

\begin{align*}
E\left(X\mid{A,S}\right) & = E \left(X\mid{S,1}\right)P\left(1\mid{A,S}\right) + E \left(X\mid{S,2}\right)P\left(2\mid{A,S}\right) \\
& = \left(\mu_1 + \tau_S\right)b + \left(\mu_2 + \tau_S\right)\left(1-b\right) \\
& = b\mu_1 + \left(1-b\right)\mu_2 + \tau_S.
\end{align*}

From here, we can calculate the expected value of the treatment effect $\tau$ as (substituting our equation for $b$ and rearranging):
  
  \begin{align*}
E\left(X\mid{A,N}\right) - E\left(X\mid{A,S}\right) & = \tau_N - \tau_S + \left(\lambda - b\right)\left(\mu_1 - \mu_2\right) \\
& = \tau_N - \tau_S - \frac{p\lambda\left(1-\lambda\right)\left(\mu_1  - \mu_2\right)}{\lambda + q\left(1-\lambda\right)},
\end{align*}

where the third term represents the bias.

Notice that if $q=1-p = 1$, then there is no bias. There is also no bias if $\mu_1 = \mu_2$, ie. if there is no difference between the disease severity groups in terms of survival time.

Assuming $\mu_1 - \mu_2 >0$, then the bias term is positive and

$$E\left(X\mid{A,N}\right)- E\left(X\mid{A,S}\right) < \tau_N - \tau_S.$$
  If $N$ is the better treatment, then $\tau_N - \tau_S>0$ and the bias will cause the trial to underplay the treatment effect. Conversely, if $S$ is better, then $\tau_N-\tau_S<0$ and the trial will exaggerate the treatment effect. Essentially, this is because more severely ill patients have been assigned to $N$ than to $S$, which reduces the average survival time for those in group $N$. 

:::
  
### Allocation bias
  
  Mathematically, allocation bias is similar to selection bias, but instead of coming from human 'error', it arises from the random process of allocation. 

Suppose a trial investigates a drug that is likely to have a much stronger effect on male patients than on female patients. The cohort of recruited participants are randomised into treatment and control groups, and it happens that there is a much smaller proportion of female patients in the treatment group than in the control group. This will distort the estimated treatment effect.

We will investigate various strategies for randomization designed to address this issue for known factors.

### Assessment bias

Measurements are made on participants throughout (and often during) the trial. These measurements will often be objective, for example the patients' weight, or concentration of blood sugar. However, some types of measurement are much more subject to the individual practitioner assessing the patient. For example, many skin conditions are assessed visually, for example estimating the proportion of the body affected. Measuring quantities such as quality of life or psychological well-being involve many subjective judgements on the part of both patient and clinician.

Clearly it is ideal for both the patient and the clinician not to know which arm of the trial the patient was part of. For treatments involving drugs, this is usually straightforward. However, for surgical interventions it is often impossible to keep a trial 'blind', and for interventions involving therapy (for example cognitive behavioural therapy) it is impossible for the patient to be unaware.



# Allocation {#sec-allocation}

Historically (and probably still, to an extent), clinical trials have not necessarily used random allocation to assign participants to groups. @altman1999treatment gives an overview of why this has led to bias, and gives some examples. @altman1999randomise and @treasure1998minimisation

Sometimes analyses compare groups in serial, so that $N_A$ patients one year (say) form the control group, and $n_B$ patients in a subsequent year, who are given treatment $B$, form the intervention group. In this scenario it is impossible to control for all other changes that have occurred with time, and this leads to a systematic bias, usually in favour of treatment $B$.

Given the need for contemporary control participants, the question becomes how to assign participants to each group. If the clinician is able to choose who receives which treatment, or if each patient is allowed to choose or refuse certain treatments, this is almost certain to introduce bias. This is avoided by using random allocation.

There are two important aspects to the allocation being *random* that we will draw attention to.

1. Every patient should have the same probability of being assigned to each treatment group.
2. The treatment group for a particular patient should not be able to be predicted.

Point 1 is important because, as we have already mentioned, the statistical theory we use to plan and analyse the trial is based on the groups being random samples from the population. 

Point 2 is important to avoid biases that come through the assignment of  a particular patient being known either in advance or after the fact. There are some approaches that 'pass' the first point, but fail at the second. As well as strict alternation ($ABABAB\ldots$), some such methods use patient characteristics such as date of birth or first letter of surname, which is not related to the trial outcome, but which enables allocations to be predicted.

We will now explore some commonly used methods of allocation. We will usually assume two equally sized groups, $A$  and $B$, but it is simple to generalize to three or more groups, or to unequal allocation.

## Allocation methods 





### Simple random allocation

Perhaps intuitively the most simple method would be a 'toin coss', where each participant has a probability 0.5 of being placed in each group. As participants arrive, assignment $A$ or $B$ is generated (with equal probability). Statistically, this scheme is ideal, since it generates the random sample we need, and the assignment of each participant is statistically independent of that of all other participants. It also doesn't require a 'master' randomisation; several clinicians can individually assign participants to treatment groups in parallel and the statistical properties are maintained. 

This method is, effectively, used in many large trials, but for small trials it can be statistically problematic. The main reason for this is chance imbalance of group sizes.

Suppose we have two groups, $A$ of size $N_A$ and $B$ of size $N_B$, with $N_A + N_B = 2n$. Patients are allocated independently with equal probability, which means

$$N_A \sim \operatorname{Bi}\left(2n,\frac{1}{2}\right), $$
  and similar for $N_B$. If the two groups are of unequal size, the larger will be of some size $N_{max}$ between $n$ and $2n$, such that for $r = n+1,\,\ldots,\,2n,$
  
  \begin{align*}
P\left(N_{max} = r\right) & = P\left(N_A = r\right) + P\left(N_B = r\right) \\
& = 2{2n \choose r}\left(\frac{1}{2}\right)^{2n}.
\end{align*}
The probability that $N_A = N_B = n$ is 

$$ P\left(N_A = N_B = n\right)= {2n \choose n}\left(\frac{1}{2}\right)^{2n}. $$
  These probabilities are shown in Figure \@ref(fig:srs-alloc). We can see that  this method leads to very unequal groups relatively easily; with $n=15$, $P\left(N_{max}\geq 20\right) = 0.099$, so there is around a one in ten chance that one group will be double or more the size of the other. 

```{r srs-alloc, echo=F, fig.cap= "The probability distribution of largest group size for n=15."}
n = 15
rvec = 15:30
pvec = sapply(
  1:length(rvec),
  function(i){
    if(rvec[i]==n){
      choose(2*n, rvec[i])* 0.5^(2*n)
    } else {
      2*choose(2*n, rvec[i])* 0.5^(2*n)  
    }
  }
  
) 
df1 = data.frame(r=rvec, p=pvec)
barplot(p~r, data=df1, xlab = "Size of largest group", ylab = "Probability")
```

As we have seen when thinking about sample sizes in Section \@ref(sec-power), this will reduce the power $\Psi$ of the trial, since it depends on $\lambda\left(N_A,\,N_B\right) = \sqrt{\frac{1}{N_A} + \frac{1}{N_B}}$. 

For larger trials, this imbalance will be less pronounced, for example Figure \@ref(fig:srsalloc200) shows the same for $n=200$.

```{r srsalloc200, echo=F, fig.cap= "The probability distribution of largest group size for n=200."}
n = 200
rvec = 200:240
pvec = sapply(
  1:length(rvec),
  function(i){
    if(rvec[i]==n){
      choose(2*n, rvec[i])* 0.5^(2*n)
    } else {
      2*choose(2*n, rvec[i])* 0.5^(2*n)  
    }
  }
  
) 
df1 = data.frame(r=rvec, p=pvec)
barplot(p~r, data=df1, xlab = "Size of largest group", ylab = "Probability")
```

In this case the $P\left(N_{max} \geq 220\right)=0.051$, so the chance of highly imbalanced groups is much lower. However, we may want to achieve balance on some factor thought to be important, for example sex, age group or disease state, and in this case there may be small numbers even in a large trial.

### Random permuted blocks

One commonly used method to randomly allocate participants while avoiding too much imbalance is to use *random permuted blocks* (RPBs). If the blocks have size $m$, and there are two groups then there are $${2m\choose m},$$ but this method can be adapted to more than two groups and to unequal group size.

If we have two groups, $A$ and $B$, then there are six *blocks* of length containing two $A$s and two $Bs$

\begin{align*}
1.& AABB\\
2.& ABAB\\
3.& ABBA\\
4.& BAAB\\
5.& BABA\\
6.& BBAA.
\end{align}

We can also randomly generate a sequence of numbers from $\left\lbrace 1, 2, 3, 4, 5, 6 \right\rbrace$, where each number has equal probability. This sequence will correspond to a sequence in $A$ and $B$ with four times the length. In this method, each patient is equally likely to receive $A$ and $B$, but there will never be a difference of more than two between the size of the two groups. 

For example, suppose the sequence begins $2,1,3,6,\ldots$. Replacing each number by its block, we have $ABAB\;AABB\;ABBA\;BBAA\;\ldots$. 

One serious disadvantage of this method is that if the block size is fixed, and the doctors involved in the trial know which participants have received which treatments (which is unavoidable in cases such as surgery), then the allocation for some patients can be perfectly predicted. This is true for the fourth in every block, and for the third and fourth if the first two were the same. This means that selection bias may be a problem in more than 25% of participants, which is deemed unacceptable; indeed, it fails our second point about randomization.

#### RPBs with random block length

The issue above can be circumvented by not only randomly choosing from a selection of blocks, but also randomly choosing the length of the block. For example, there are 
$$ {6 \choose 3} = 20$$
possible blocks of size 6. Instead of always selecting from the six possible 4-blocks, a sampling scheme can be as follows.

  1. A random number $X$ is drawn from $\left\lbrace 4,6\right\rbrace$ to select the block length. 
  2. A second random number $Y$ is drawn from 1 to 6 (if the block length is four) or 1 to 20 (if the block length is 6).
  3. The block corresponding to $Y$ is chosen and participants assigned accordingly. 
  4. If more participants are needed, go back to step 1.
  
As well as ensuring that patients are equally likely to receive treatments $A$ and $B$, and that $N_A$ and $N_B$  can never differ by more than three, this method hugely reduces the possibility of enabling selection bias. The assignment of a patient can only be perfectly predicted if the difference is three, and this happens only for two of the twenty blocks of length six.

### Biased coin designs and urn schemes

It may be that we prefer a method which achieves balance while retaining the pure stochasticity of simple random sampling. An advantage of RPBs was that once the sequence was generated, no computing power was needed. However, it is safe now to assume that any hospital pharmacy, nurse's station, GP office or other medical facility will have a computer with access to the internet (or some internal database), and therefore more sophisticated methods are available.

Biased coin designs and urn schemes both work by adjusting the probabilities of allocation according to balance of the design so far, such that a participant is less likely to be assigned to an over-represented group.

#### Biased coin designs

Suppose we are using a biased coin design for a trial to compare two treatments, $A$ and $B$. At the point where some number $n$ (not the total trial cohort) have been allocated, we can use the notation $N_A\left(n\right)$ for the number of participants allocated to treatment $A$, and $N_B\left(B\right)$ for the number of participants allocated to treatment $B$. Using these, we can denote the *imbalance* in treatment numbers as 

$$ D\left(n\right) = N_A\left(n\right) - N_B\left(n\right) = 2N_A\left(n\right) - n.$$
  We use the imbalance $D\left(n\right)$ to alter the probability of allocation to each treatment in order to restore (or maintain) balance in the following way:
  
  - If $D\left(n\right)=0$, allocate patient $n+1$ to treatment $A$ with probability $\frac{1}{2}$.
- If $D\left(n\right)<0$, allocate patient $n+1$ to treatment $A$ with probability $P$.
- If $D\left(n\right)>0$, allocate patient $n+1$ to treatment $A$ with probability $1-P$.

where $P\in\left(\frac{1}{2}, 1\right)$.

> Question: What would happen if $P=\frac{1}{2}$ or $P=1$?
  
  If, at some point in the trial, we have $\lvert D\left(n\right)\rvert = j$, for some $j>0$, then we must have either

$$ \lvert D\left(n+1\right)\rvert = j+1 $$
  or 
$$ \lvert D\left(n+1\right)\rvert = j-1 .$$
  Because of the way we have set up the scheme, 

$$ p\big(\lvert D\left(n+1\right)\rvert = j+1\big) = 1-P $$
  and 

$$ p\big(\lvert D\left(n+1\right)\rvert = j-1\big) = P.$$
  If $\lvert D\left(n\right) \rvert = 0$, ie. the scheme is in exact balance after $n$ allocations, then we must have $\lvert D\left(n\right)\rvert = 1$. 

The absolute imbalances therefore form a simple random walk on the non-negative integers, with transition probabilities


$$
  \begin{aligned}
P\big(\lvert D\left(n+1\right) \rvert = 1 \mid \lvert D\left(n\right)=0\big)& = 1\\
P\big(\lvert D\left(n+1\right) \rvert = j+ 1 \mid \lvert D\left(n\right)=j\big)& = 1 - P\\
P\big(\lvert D\left(n+1\right) \rvert = j-1 \mid \lvert D\left(n\right)=j\big)& = P
\end{aligned}
$$
  Figure \@ref(fig:biasedcoin-p2thirds) shows four realisations of this random walk with $P=0.667$. We see that sometimes the imbalance gets quite high, but in general it isn't too far from 0.

```{r biasedcoin-p2thirds, echo=F, fig.cap = "Absolute imbalance for a biased-coin scheme  with $P=0.667$."}

biased.coin = function(
  n,     # how many allocations to do
  p,      # the probability P as above
  seed = 60003
){
  set.seed(seed)
  abs_Dn = c(1)     # the first imbalance has to be one
  
  for (i in 2:n){
    if (abs_Dn[i-1] == 0){
      abs_Dn[i] = 1 
    } else {
      abs_Dn[i] = sample(c(abs_Dn[i-1]-1, abs_Dn[i-1]+1), size = 1, prob = c(p, 1-p))
    }
  }
  Dn_df = data.frame(
    ID = 1:n,
    abs_Dn = abs_Dn
  )
  Dn_df
}

abs_50_23a = biased.coin(n=50, p=0.667)
plot1 = ggplot(data=abs_50_23a, aes(x=ID, y=abs_Dn)) + 
  geom_line() + geom_point(col=2) + 
  xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 8)

abs_50_23b = biased.coin(n=50, p=0.667, seed = 2863727)
plot2 = ggplot(data=abs_50_23b, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance")+
  ylim(0, 8)

abs_50_23c = biased.coin(n=50, p=0.667, seed = 2313467)
plot3 = ggplot(data=abs_50_23c, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance")+
  ylim(0, 8)

abs_50_23d = biased.coin(n=50, p=0.667, seed = 2682144)
plot4 = ggplot(data=abs_50_23d, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance")+
  ylim(0, 8)


grid.arrange(plot1, plot2, plot3, plot4, nrow=2)

```


Figure \@ref(fig:p-nearlyhalf) shows four realisations of the random walk with $P=0.55$. Here, the imbalance is able to get very high (note the change in $y$-axis); for example in the first plot, if we stopped the trial at $n=50$ we would have 34 participants in one arm and only 16 in the other.

```{r p-nearlyhalf, fig.cap = "Absolute imbalance for a biased-coin scheme  with $P = 0.55$."}
abs_50_55a = biased.coin(n=50, p=0.55)
plot1 = ggplot(data=abs_50_55a, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 18)

abs_50_55b = biased.coin(n=50, p=0.55, seed = 2863727)
plot2 = ggplot(data=abs_50_55b, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 18)

abs_50_55c = biased.coin(n=50, p=0.55, seed = 2313467)
plot3 = ggplot(data=abs_50_55c, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 18)

abs_50_55d = biased.coin(n=50, p=0.55, seed = 2682144)
plot4 = ggplot(data=abs_50_55d, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 18)


grid.arrange(plot1, plot2, plot3, plot4, nrow=2)

```

By contrast, with $P=0.9$ as in Figure \@ref(fig:p-nearlyone), there is much less imbalance. However, this brings with it greater predictability. Although allocation is always random, given some degree of imbalance (likely to be known about by those executing the trial), the probability of guessing the next allocation correctly is high (0.9). This invites the biases we have been trying to avoid, albeit in an imperfect form.

```{r p-nearlyone, fig.cap = "Absolute imbalance for a biased-coin scheme  with $P = 0.9$."}
abs_50_9a = biased.coin(n=50, p=0.9)
plot1 = ggplot(data=abs_50_9a, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 4) 

abs_50_9b = biased.coin(n=50, p=0.9, seed = 2863727)
plot2 = ggplot(data=abs_50_9b, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 4) 

abs_50_9c = biased.coin(n=50, p=0.9, seed = 2313467)
plot3 = ggplot(data=abs_50_9c, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 4) 

abs_50_9d = biased.coin(n=50, p=0.9, seed = 2682144)
plot4 = ggplot(data=abs_50_9d, aes(x=ID, y=abs_Dn)) + geom_line() + geom_point(col=2) + xlab("Participant ID") + ylab("Absolute Imbalance") +
  ylim(0, 4) 


grid.arrange(plot1, plot2, plot3, plot4, nrow=2)
```

A big disadvantage to the biased coin scheme is that the same probability is used regardless of the size of the imbalance (assuming it isn't zero). In the next section, we introduce a method where the probability of allocating the next patient to the underrepresented treatment gets larger as the imbalance grows.

#### Urn models

*Urn models* for treatment allocation use urns in the way that you might well remember from school probability (or indeed often we had drawers of socks). In this setting, the urn starts off with a ball for each treatment, and a ball is added to the urn each time a participant is allocated. The ball is labelled according to the treatment allocation that participant **did not** receive. 

To allocate the next participant, a ball is drawn from the urn. If the allocations at this point are balanced, then the participant has equal probability of being allocated to each treatment. If there is imbalance, there will be more balls labelled by the underrepresented treatment, and so the participant is more likely to be allocated to that one. The greater the imbalance, the higher the probability of reducing it. 

The process described so far is a $UD\left(1,1\right)$; there is one ball for each treatment to start with, and one ball is added to the urn after each allocation. To be more general, we can assume a $UD\left(r,s\right)$ scheme. Now, there are $r$ balls for each treatment in the urn to begin with, and $s$ are added after each allocation.

Near the start of the allocation, the probabilities are likely to change a lot to address imbalance, but once a 'reasonable number' of allocations have been made it is likely to settle into simple random sampling (or very close).

Once again, we can find the transition probabilities by considering the absolute imbalance $\lvert D\left(n\right) \rvert$.

Suppose that after participant $n$, $N_A\left(n\right)$ participants have been allocated to treatment $A$, and $N_B\left(n\right) = n - N_A\left(n\right)$ to treatment $B$. The imbalance is therefore

$$D\left(n\right) = N_A\left(n\right) - N_B\left(n\right) = 2N_A\left(n\right) - n.$$
  After $n$ allocations there will be $2r + ns$ balls in the urn: $r$ for each treatment at the start, and $s$ added after each allocation. Of these, $r + N_B\left(n\right)s$ will be labelled by treatment $A$ and $r + N_A\left(n\right)s$ by treatment $B$.

To think about the probabilities for the absolute imbalance $\lvert D\left(n\right)\rvert$, we have to be careful now about which direction it is in. If the trial currently (after allocation $n$) has an imbalance of participants in favour of treatment $A$, then the probability that it becomes less imbalanced at the next allocation is the probability of the next allocation being to treatment $B$, which is

$$
  \begin{aligned}
p\left(\lvert D\left(n+1\right)\rvert = j-1 \mid D\left(n\right)=j, j>0\right) & = \frac{r + N_A\left(n\right)s}{2r + ns} \\
& = \frac{r + \frac{1}{2}\left(n + D\left(n\right)\right)s}{2r + ns} \\
& = \frac{1}{2} + \frac{D\left(n\right)s}{2\left(2r + ns\right)} \\
& = \frac{1}{2} + \frac{\lvert D\left(n\right)\rvert s}{2\left(2r + ns\right)}.
\end{aligned}
$$
  Similarly, if there is currently an excess of patients allocated to treatment $B$, then the imbalance will be reduced if the next allocation is to treatment $A$, and so the conditional probability is

$$
  \begin{aligned}
p\left(\lvert D\left(n+1\right)\rvert = j-1 \mid D\left(n\right)=j, j<0\right) & = \frac{r + N_B\left(n\right)s}{2r + ns} \\
& = \frac{r + \frac{1}{2}\left(n - D\left(n\right)\right)s}{2r + ns} \\
& = \frac{1}{2} - \frac{D\left(n\right)s}{2\left(2r + ns\right)}\\
& = \frac{1}{2} + \frac{\lvert D\left(n\right)\rvert s}{2\left(2r + ns\right)}.
\end{aligned}
$$
  
  Because the process is symmetrical, an imbalance of a given magnitude (say $\lvert D\left(n\right)\rvert=j$) is equally likely to be in either direction. That is

$$p\big(D\left(n\right) < 0 \mid \lvert D\left(n\right)\rvert =j \big)= p\big(D\left(n\right) > 0 \mid \lvert D\left(n\right)\rvert =j \big) = \frac{1}{2}.$$
  
  Therefore we can use the law of total probability (or partition theorem) to find that 

$$
  p\big(\lvert D\left(n+1\right) \rvert = j-1 \mid \lvert D\left(n\right) \rvert = j \big) = \frac{1}{2}  + \frac{\lvert D\left(n\right)\rvert s}{2\left(2r + ns\right)}.
$$
  Since the two probabilities are equal this is trivial. Since the only other possibility is that the imbalance is increased by one, we also have

$$p\big(\lvert D\left(n+1\right) \rvert = j+1 \mid \lvert D\left(n\right) \rvert = j \big) = \frac{1}{2}  - \frac{\lvert D\left(n\right)\rvert s}{2\left(2r + ns\right)}. $$
  As with the biased coin design, we also have the possibility that the imbalance after $n$ allocations is zero, in which case the absolute imbalance after the next allocation will definitely be one. This gives us another simple random walk, with

$$
  \begin{aligned}
P\big(\lvert D\left(n+1\right) \rvert = 1 \mid \lvert D\left(n\right)=0\big)& = 1\\
P\big(\lvert D\left(n+1\right) \rvert = j+ 1 \mid \lvert D\left(n\right)=j\big)& = \frac{1}{2}  - \frac{\lvert D\left(n\right)\rvert s}{2\left(2r + ns\right)}\\
P\big(\lvert D\left(n+1\right) \rvert = j-1 \mid \lvert D\left(n\right)=j\big)& = \frac{1}{2}  + \frac{\lvert D\left(n\right)\rvert s}{2\left(2r + ns\right)}
\end{aligned}
$$
  
```{r}
urn = function(r,s,N, seed = 60003){
  Dn = c(1)    # the first imbalance has to be one
  for (n in 2:N){
    p_inc = 0.5 - (Dn[n-1]*s)/(2*(2*r+(n-1)*s))
    p_dec = 0.5 + (Dn[n-1]*s)/(2*(2*r+(n-1)*s))
    if (Dn[n-1]==0){
      Dn[n] = 1
    } else {
      Dn[n] = sample(
        c(Dn[n-1]+1, Dn[n-1]-1), 
        size=1, 
        prob = c(p_inc, p_dec))
    }
    
  }
  Dn_df = data.frame(ID = 1:N, Dn = Dn)
  Dn_df
}

```


```{r urn11, fig.cap = "Four realisations of absolute imbalance for r=1, s=1, N=50."}

urn1 = urn(1,1,50)
urn2 = urn(1,1,50, seed = 6004)
urn3 = urn(1,1,50, seed = 5678)
urn4 = urn(1,1,50, seed = 431)

plot1 = ggplot(data=urn1, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance") +ylim(0,10)
plot2 = ggplot(data=urn2, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,10)
plot3 = ggplot(data=urn3, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,10)
plot4 = ggplot(data=urn4, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,10)

grid.arrange(plot1, plot2, plot3, plot4, nrow=2)


```

```{r urn18, fig.cap = "Four realisations of absolute imbalance for r=1, s=8, N=50."}

urn1 = urn(1,8,50)
urn2 = urn(1,8,50, seed = 6004)
urn3 = urn(1,8,50, seed = 5678)
urn4 = urn(1,8,50, seed = 431)

plot1 = ggplot(data=urn1, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,15)
plot2 = ggplot(data=urn2, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,15)
plot3 = ggplot(data=urn3, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,15)
plot4 = ggplot(data=urn4, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,15)

grid.arrange(plot1, plot2, plot3, plot4, nrow=2)


```

```{r urn81, fig.cap = "Four realisations of absolute imbalance for r=8, s=1, N=50."}

urn1 = urn(8,1,50)
urn2 = urn(8,1,50, seed = 6004)
urn3 = urn(8,1,50, seed = 5678)
urn4 = urn(8,1,50, seed = 431)

plot1 = ggplot(data=urn1, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,8)
plot2 = ggplot(data=urn2, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,8)
plot3 = ggplot(data=urn3, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,8)
plot4 = ggplot(data=urn4, aes(x=ID, y=Dn)) + geom_line() +
  geom_point(col=2) + xlab("Participant ID") + 
  ylab("Absolute Imbalance")+ylim(0,8)

grid.arrange(plot1, plot2, plot3, plot4, nrow=2)


```

We see that imbalance is reduced, particularly for small $n$. A small $r$ and large $s$ enhance this, since the large number ($s$) of balls added to the urn with each allocation weight the probabilities more heavily, as in Figure \@ref(fig:urn18). By contrast, if $r$ is large and $s$ is small, as in Figure \@ref(fig:urn81), the probabilities stay closer to $\left(\frac{1}{2}, \frac{1}{2}\right)$ and so more imbalance occurs early on.


## Problems with allocation


In clinical trials papers, the allocation groups are usually summarised in tables giving summary statistics (eg. mean and SD) of each characteristic for the control group and the intervention group. The aim of these is to show that the groups are similar enough for any difference in outcome to be attributed to the intervention itself. Figure \@ref(fig:licorice-participants) shows an example.

```{r licorice-participants, echo=FALSE, out.width="60%", fig.cap = "Summary statistics for an RCT comparing a licorice gargle (the intervention) to a sugar-water gargle (the standard). From @ruetzler2013randomized"}
include_graphics("images/licorice.jpg")
```

The problem here is that only the marginal distributions are compared for similarity. Consider the following (somewhat extreme and minimalistic) scenario. A study aims to investigate the effect of some treatment, and to balance for gender and age in their allocation, resulting in the following summary table.

```{r}

tab = data.frame(
  Variable = c("Gender (female, %)", "Age, y (mean +/- SD)"),
  Control  = c("50", "50 +/- 10"),
  Intervention = c("50", "50 +/- 10")
)

names(tab) = c("Variable", "Control (n=100)", "Intervention (n=100)")

knitr::kable(tab)

```

This appears to be a perfectly balanced design. However, if we look at the joint distribution, we see that there are problems.


```{r}
set.seed(60103)
control_df = data.frame(
  id = 1:100,
  Gender = as.factor(c(rep("Female",50), rep("Male",50))),
  Age = sort(rnorm(100, mean=50, sd=10))
)

intervention_df = data.frame(
  id = 1:100,
  Gender = as.factor(c(rep("Male",50), rep("Female",50))),
  Age = sort(rnorm(100, mean=50, sd=10))
)

plot_control  = ggplot(data=control_df, aes(x=Age, fill=Gender)) + 
  geom_histogram(position = "dodge") + ggtitle("Control group") +
  theme(legend.position="bottom")
plot_intervention  = ggplot(data=intervention_df, aes(x=Age, fill=Gender)) +
  geom_histogram(position = "dodge") + ggtitle("Intervention group") +
  theme(legend.position="bottom")

grid.arrange(plot_control, plot_intervention, nrow=1)
```


```{r}

mean_male_cont = round(mean(control_df$Age[control_df$Gender == "Male"]),2)
mean_male_int = round(mean(intervention_df$Age[intervention_df$Gender == "Male"]),2)
mean_female_cont = round(mean(control_df$Age[control_df$Gender == "Female"]),2)
mean_female_int = round(mean(intervention_df$Age[intervention_df$Gender == "Female"]),2)

sd_male_cont = round(sd(control_df$Age[control_df$Gender == "Male"]),2)
sd_male_int = round(sd(intervention_df$Age[intervention_df$Gender == "Male"]),2)
sd_female_cont = round(sd(control_df$Age[control_df$Gender == "Female"]),2)
sd_female_int = round(sd(intervention_df$Age[intervention_df$Gender == "Female"]),2)


df = data.frame(
  Male = c(
    sprintf("%s (%s)", mean_male_cont, sd_male_cont), 
    sprintf("%s (%s)", mean_male_int, sd_male_int)),
  Female = c(
    sprintf("%s (%s)", mean_female_cont, sd_female_cont), 
    sprintf("%s (%s)", mean_female_int, sd_female_int))
)
row.names(df) = c("Control", "Intervention")

knitr::kable(df)%>%kableExtra::column_spec(1, bold = TRUE)

```

If the intervention is particularly effective in older men, our trial will not notice. Likewise, if older women generally have a more positive outcome than older men, our trial may erroneously find the intervention to be effective.

Although this example is highly manufactured and [hopefully!] unlikely to take place in real life, for clinical trials there are often many demographic variables and prognostic factors being taken into account. Achieving joint balance across all them is very difficult, and extremely unlikely to happen if it isn't aimed for. @treasure1998minimisation give an example in relation to a hypothetical study on heart disease 

> Supposing one group has more elderly women with diabetes and symptoms of heart failure. It would then be impossible to attribute a better outcome in the other group to the beneficial effects of treatment since poor left ventricular function and age at outset are major determinants of survival in any longitudinal study of heart disease, and women with diabetes, as a group, are likely to do worse. At this point the primary objective of randomisation—exclusion of confounding factors—has failed. ... If a very big trial fails, because, for example, the play of chance put more hypertensive smokers in one group than the other, the tragedy for the trialists, and all involved, is even greater.


## Stratified sampling

The usual method of achieving balance with respect to prognostic factors is to divide each factor into several levels and to consider treatment assignment separately for patients having each particular combination of such factor levels. Such groups of patients are commonly referred to as randomization groups or strata. Treatment assignment is performed entirely separately for each stratum, a permuted block design of the type mentioned above often being used. In fact, using purely random treatment assignment for each stratum is equivalent to simple random assignment, so that some equalization of treatment numbers within each stratum is essential. This whole procedure is analogous to performing a factorial experiment, without being able to control the factor levels of the experimental units.

:::{.example}

Suppose we are planning a trial involving people over the age of 50, and we anticipate that age and sex might both play an important role in how participants respond to the treatment.

For sex, we use the levels 'male' and 'female', and for age we split the range into 50-65, 66-80 and 81 or over. We therefore have six strata, and we use an allocation strategy independently in each stratum. For example, below we have used randomly permuted blocks of length four.

```{r}
df_strat = data.frame(
  Male = c("ABAB BBAA ...", "BAAB AABB ...",  "ABAB ABBA ..."),
  Female = c("ABBA BBAA ...", "BABA BAAB ...", "ABBA BAAB ..."))
row.names(df_strat) = c("50-65", "66-80", "81 and over")
knitr::kable(df_strat)%>%kableExtra::column_spec(1, bold = TRUE)
```


Each time a new participant arrives, we follow the randomization pattern for their stratum. We could use another allocation scheme within each stratum, for example an urn model or a biased coin. It is important that we use one that aims to conserve balance, or else the benefits of stratification are lost.
:::


A difficulty with stratified sampling is that the number of strata can quickly become large as the number of factors (or the number of levels within some factors) increases. For example, if we have four prognostic factors each with three levels, there are $3^4=81$ strata. This creates a situation that is at best unwieldy, and at worst completely unworkable; in a small trial (with say 100 patients in each arm) there may be some strata with no patients in (this is actually not a problem), and probably many more with only one (this is much more problematic). 

### Restricted randomization

READ UP ON THIS!

## Minimization

@altman1999randomise tees up minimization.
@treasure1998minimisation a good source.

Minimization was first proposed by @taves1974minimization, then shortly after by @pocock1975sequential and @freedman1976use. The aim of minimization is to minimize the difference between the two groups. The people running the trial must first specify all of the factors they would like to be balanced between the two groups. These should be any variables that are thought to possibly affect the outcome. As an example, in a study comparing aspirin to a placebo preceding coronary artery surgery, @kallis1994pre chose age, sex, operating surgeon, number of coronary arteries affected and left ventricular function.

When a patient enters the trial, these factors are listed. The patient is then allocated in such a way as to minimise any difference in these factors. The minimization method has evolved since its conception, and exists in several forms. Two areas in which methods vary are

  * Whether continuous variables have to be binned
  * Whether there is any randomness
  
It is generally agreed that if the risk of selection bias cannot be avoided, there should be an element of randomness. It is also usually accepted that if a variable is included in the minimization, it should also be included in the statistical analysis.

### Minimization algorithm from @pocock1975sequential

Suppose we have a trial in which patients are recruited sequentially and need to be allocated to a trial arm. Suppose there are $N$ treatment arms and $M$ prognostic factors over which we require balance, and that these $M$ factors have $n_1,\ldots,n_M$ levels. 

We describe the minimization allocation procedure by considering an arbitrary point in the trial, at which a patient has just materialised with levels $r_1,\ldots,r_M$ of the $M$ prognostic factors. At this point we use $x_{ijk}$ to denote the number of patients with level $j$ of factor $i$ who have been assigned treatment $k$, for $j=1,2,\ldots,n_i;\,i=1,2,\ldots,M$ and $k=1,\ldots,N$.

For each treatment $k$, we consider the new $\left\lbrace x_{ijl}\right\rbrace,$ which we write as $\left\lbrace x_{ijl}^k\right\rbrace$, that we would have if we assigned this patient to treatment arm $k$. That is

$$
\begin{aligned}
x_{ijl}^k & = x_{ijl}   \text{ for }j\neq{r_i}\text{ or }l\neq{k}\\
x_{i{r_i}k}^k & = x_{i{r_i}k}+1
\end{aligned}
$$
Now imagine we have some function 

$$\operatorname{D}\left(\left\lbrace z_l\right\rbrace_{l=1}^N\right)$$ that measures the ``amount of variation'' in any set of non-negative integers $(\left\lbrace z_l\right\rbrace_{l=1}^N$, so that

$$d_{ik} = \operatorname{D}\left(\left\lbrace x_{i{r_i}l}^k\right\rbrace_{l=1}^N\right) $$
is the `lack of balance' of treatment assignments for patients with level $r_i$ of factor $i$. Next we need some function 

$$\operatorname{G}:\mathbb{R}^M \rightarrow \mathbb{R}$$
  so that 

$$G_k = \operatorname{G}\left(d_{1k},\ldots,d_{Mk}\right) $$
  represents the ``total amount of imbalance'' in treatments numbers over all factor levels, if treatment $k$ were assigned to this new patient.


## Some simulated examples

To conclude this section we will demonstrate methods using simulated datasets with the same underlying probability distributions. We consider four factors:
  
  * Age: 44 and under, 45 - 54, 55-64, 65-74 and 75 and over (5 levels)
* Sex: M or F
* Smoking: current, past or never
* Hypertension: Yes or no

These will be generated using the following distributions:
  
  * Age: $N\left(60, 10^2\right)$ (then binned as above)
* Sex: $p\left(M\right)= p\left(F\right) = 0.5$
  * Smoking: $p\left(\text{Current}\right)=0.4,\,p\left(\text{Past}\right) = p\left(\text{Never}\right) = 0.3$
  * Hypertension: 	
  $p(Y) = \begin{cases}
0.25  & \text{Age under 45} \\
0.55 & \text{Age 45 to 74}\\
0.75 & \text{Age 75 and over}
\end{cases}$
  
  To demonstrate our allocation methods, we simulate one dataset from the distribution described, containing 100 participants. This dataset is shown in the Table below, and in Figures \@ref(fig:part-gen1a) and \@ref(fig:part-gen1b).


```{r}
library(rmarkdown)
participant_gen = function(n, seed = 4468){
  set.seed(seed)
  data_df = data.frame(matrix(NA, nrow=n, ncol=5))
  names(data_df) = c("ID", "Sex", "Age", "Smoking", "Hypertension")
  Age_raw = rnorm(n, mean=60, sd=10)
  age_binned = sapply(1:n, function(i){
    if(Age_raw[i] < 45){
      "44 and under"
    } else if (Age_raw[i] < 55){
      "45 to 54"
    } else if (Age_raw[i] < 65){
      "55 to 64"
    } else if (Age_raw[i] < 75){
      "65 to 74"
    } else {"75 and over"}
  })
  data_df$Age = as.factor(age_binned)
  data_df$Sex = as.factor(sample(c("Female", "Male"), size=n, replace=T))
  
  smoking_raw = runif(n=n)
  smoking_cat = rep(NA, n)
  smoking_cat[smoking_raw<0.4] = "Current"
  smoking_cat[smoking_raw >=0.4 & smoking_raw < 0.7] = "Past"
  smoking_cat[smoking_raw>=0.7] = "Never"
  
  data_df$Smoking = as.factor(smoking_cat)
  
  hypertension = sapply(
    1:n,
    function(i){
      if(Age_raw[i]<45){
        sample(c("Yes", "No"), size=1, prob=c(0.25, 0.75))  
      } else if(Age_raw[i] < 75){
        sample(c("Yes", "No"), size=1, prob=c(0.55, 0.45))
      } else {
        sample(c("Yes", "No"), size=1, prob=c(0.75, 0.25))
      }
    })
  
  data_df$Hypertension = as.factor(hypertension)
  data_df$ID = 1:n
  
  data_df
  
}

test_df = participant_gen(100)

paged_table(test_df)

```


```{r part-gen1a, echo=F, fig.cap = "A simulated dataset of 100 participants, using the distributions described above, split by hypertension, age and sex."}

ggplot(data=test_df, aes(x=Hypertension, fill=Sex)) + geom_bar(col=1) + facet_wrap(~Age, nrow=1) +
  theme(legend.position = "bottom")

```



```{r part-gen1b,  echo=F,  fig.cap = "A simulated dataset of 100 participants, using the distributions described above, split by smoking history, age and sex."}

ggplot(data=test_df, aes(x=Smoking, fill=Sex)) + geom_bar(col=1) + facet_wrap(~Age, nrow=1) +
  theme(legend.position = "bottom")

```

### Simple random allocation

In simple random allocation, each participant is allocated to one of the two trial arms with equal probability. In terms of our allocation, 




# The intervention


Having settled on a sample size and an allocation strategy, the trial can now be run.



